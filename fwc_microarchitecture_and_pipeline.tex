% chapter included in forwardcom.tex
\documentclass[forwardcom.tex]{subfiles}
\begin{document}
\RaggedRight

\chapter{Microarchitecture and pipeline design}
The ForwardCom instruction set is intended to facilitate a consistent and efficient design of the pipeline of a superscalar microprocessor. Instructions can have no more than one destination operand, up to three or four source operands, a mask register, a fallback register, and a register specifying vector length. The last source operand can be a register, a memory operand or an immediate constant. All other operands are registers, except for memory write instructions. The total number of input registers to an instruction, including source operands, mask, fallback, memory base pointer, index, and vector length specifier cannot exceed four, or optionally five.
\vspace{2mm}

No instruction can have more than one memory operand. No instruction can have both a memory source operand and an immediate operand, though this may be allowed in future extensions. Any extra immediate operand field can be used for option bits. 
\vspace{2mm}

A high performance pipeline may be designed as superscalar with the following stages.
\begin{itemize}
\item  Fetch. Fetching blocks of code from the instruction cache, one cache line at a time, or as determined by the branch prediction machinery. 

\item  Instruction length decode. Determine the length of each instruction and identify tiny instructions. Distribute the first P instructions into each their pipeline lane, where P is the number of parallel lanes implemented in the pipeline. Excess instructions may be queued for the next clock cycle. The length of an instruction is determined by two bits of the first code word in order to simplify this process.

\item  Instruction decode. Identify and classify all operands, opcode and option bits. Determine input and output dependencies. A consistent template system simplifies this step.

\item  Register allocation and renaming. 

\item  Instruction queue. 

\item  Put instructions into reservation station. Schedule for address calculator. 

\item  Calculate address and length of memory operand. Check access rights. Do calculations that depend on immediate operand only.

\item  Read memory operand. Schedule for execution units.  

\item  Execution units.  

\item  Retire or branch. 
\end{itemize}

It is not necessary to split instructions into micro-operations if the reading of memory operands is done in a separate pipeline stage and instructions are allowed to stay in the reservation station until the memory operand has been read. 
\vspace{2mm}

Each stage in the pipeline should ideally require only one clock cycle. Instructions waiting for an operand should stay in the reservation station. Most instructions will use only one clock cycle in the execution unit. Multiplication and floating point addition need a pipelined execution unit with several stages. Division and square root may use a separate state machine. 
\vspace{2mm}

Jump, branch, call, and return instructions also fit into this pipeline design. 
\vspace{2mm}

The reservation station has to consider all the input and output dependencies of each instruction. Each instruction can have up to four or five input dependencies and one output dependency. 
\vspace{2mm}

There may be multiple execution units so that it is possible to run multiple instructions in the same clock cycle if their operands are independent. 
\vspace{2mm}

An efficient out-of-order processing requires renaming of the general purpose registers and vector registers, but not necessarily the special registers. 
\vspace{2mm}

Complex instructions and microcode should generally be avoided. We do not have an instruction for saving or restoring all registers during a task switch. Instead, the necessary instructions for saving and restoring registers are implemented as tiny instructions to reduce the size of an instruction sequence that saves all registers. 
\vspace{2mm}

The following instructions are moderately complex: call, return, div, rem, sqrt, cmp\_swap, save\_cp, restore\_cp. These instructions may be implemented as dedicated state machines. The same applies to traps, Interrupts and system calls. 
\vspace{2mm}

Some current CPUs have a ``stack engine'' in order to predict the value of the stack pointer for a push, pop or call instruction when preceding stack operations are delayed due to operands that are not available yet. Such a system is not needed if we have a dual stack design (see page \pageref{dualStack}). Even with a single stack design, there is little need for a stack engine because push and pop operations will be rare in critical parts of the code if the function calling conventions in this document are followed (page \pageref{functionCallingConventions}). 
\vspace{2mm}

Branch prediction is important for the performance. We may implement four different branch prediction algorithms: one for ordinary branches, one for loops, one for indirect jumps, and one for function returns. The long form of branch instructions have an option bit for indicating loop behavior. The short form of branch instructions does not have space for such a bit. The initial guess may be to assume loop behavior if the branch goes backwards and ordinary branch behavior if the branch goes forwards. This assumption may be corrected later, if necessary, by the branch prediction machinery. 
\vspace{2mm}

The code following a branch is executed speculatively until it is determined whether the prediction was right. We may implement features for running both sides of a branch speculatively at the same time. 
\vspace{2mm}

The ForwardCom design allows large microprocessors with very long vector registers. This requires special design considerations. The chip layout of vector processors is typically divided into ``data lanes'' so that the vertical transfer of data from a vector element to the corresponding 
vector element in another vector (i. e. same lane) is faster than the horizontal transfer of data from one vector element to another element at another position of the same vector (i. e. different lane). This means that instructions that transfer data horizontally across a vector, such as broadcast and permute instructions, may have longer latencies than other vector instructions. The scheduler needs to know the instruction latency, and this can be a problem if the latency depends on the distance of data transfer on very long vectors. This problem is addressed by indicating the vector length or the distance of data transfer for such instructions in a separate operand, which always uses the RS register field. This information may be redundant because the vector length is stored in the vector register operands, but the scheduler needs this information as early as possible. The other register operands are typically not ready until the clock cycle where they go to the execution unit, while the vector length is typically known earlier. The microprocessor can read the RS register at the address calculation stage in the pipeline, where it also reads any pointer, index register and vector length for memory operands. This allows the scheduler to predict the latency a few clock cycles in advance. The instruction set provides the extra information about vector length or data transfer length in RS for all instructions that involve horizontal data transfer, including memory broadcast, permute, insert, extract and shift instructions, but not broadcasting of immediate constants. 
\vspace{2mm}

The data path to the data cache and memory should be quite wide, possibly matching the maximum vector length, because cache access and memory access are typical bottlenecks.


\section{Proposal for reducing branch misprediction delay}
Modern superscalar processors often have quite long pipelines. This gives a long branch misprediction delay. The branch misprediction delay is normally equal to the number of pipeline stages from a branch instruction is fetched till it is executed.
\vspace{2mm}

It may be possible to reduce this delay by executing branch instructions as early as possible in the pipeline. Any preceding instructions that a branch instruction depends on may also be executed early. It is proposed to execute all control transfer instructions (branch, jump, call, and return) in the front end of the pipeline, before the register renaming and scheduler. Any preceding instruction that a branch depends on, could likewise be moved to the front end. This idea is somewhat similar to the principle described in the article:

Sheikh, Rami, James Tuck, and Eric Rotenberg. “Control-Flow Decoupling.” In Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture, 329–340. IEEE Computer Society, 2012.
\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.952.6936&rep=rep1&type=pdf}
\vspace{2mm}

A problem with the previously proposed methods for decoupling control flow from execution is that it is complicated for the CPU to determine which instructions belong to the control flow. Here, we will propose to tell the CPU in advance which registers are used for the control flow. This may be one or more general purpose registers which are used for loop counters, etc.
\vspace{2mm}

The vision is this: There are two sets of ALU's, one in the front end used for control flow, and one in the back end used for ordinary calculations. The front end does not use register renaming, but relies on the permanent register file.
\vspace{2mm}

The compiler is indicating which registers it intends to use for control flow. For example, it may use register r1 for loop counter, r2 for a branch condition, and r3 for index into a jump table. The compiler issues an instruction to indicate that r1, r2, and r3 are marked for executing in the front end. 
All instructions that modify r1, r2, and r3, as well as all control transfer instructions, will then execute in the front end if possible. The remaining instructions are sent to the the back end. Instructions that have any of these registers as input operand will have the register operand replaced by its value before it is sent to the back end. Other register operands that are available in the permanent register file can likewise be replaced by their values. Instructions that execute in the front end may write their result not only to the permanent register file but also to any pending instruction that needs it.
\vspace{2mm}

The result of this mechanism is that a loop that is controlled only by the marked registers
will be unrolled in the front end. The instructions in the loop body will be passed on to the back end with the loop counter replaced by its value. The back end has register renaming so that it can execute the body of the second iteration before it has finished the body of the first iteration, etc.
\vspace{2mm}

All other control flow, such as branches, jumps, calls, returns, etc., will be unrolled in the front end as well so that control flow is effectively resolved before execution as far as it does not depend on delayed data.
\vspace{2mm}

The front end supports a minimum of out-of-order execution in the sense that an instruction that is waiting for an operand should not delay any independent subsequent instructions. But the front end does not need a complicated structure with a long queue and scheduler.
\vspace{2mm}

In case the front end ALU is idle because there are no instructions that write to marked registers, it may execute any other instructions if their operands are ready.
\vspace{2mm}

The front end should have access to the permanent register file, while the back end uses temporary registers with register renaming. The renamed registers will be written to the permanent register file when they retire. We have to keep track of whether the permanent registers are up to date. When the decoder sees an instruction that writes to a register, it will mark this register as invalid in the permanent register file and remember which instruction it belongs to. When this instruction is executed (whether in the front end or the back end), the register entry is marked as valid, unless another instruction writing to the same register has been seen in the meantime.
\vspace{2mm}

The front end may fetch and decode speculatively, but not execute speculatively. It may even fetch both sides of a branch that it expects to have poor prediction. Fetching both sides of a branch becomes cheaper here than with a traditional superscalar design because it only needs to duplicate two pipeline stages (fetch and decode) in order to minimize branch misprediction bubbles.
\vspace{2mm}

The fetch and decode stages should have higher throughput than the rest of the pipeline so that it can catch up and fill the pipeline after a branch bubble. The fetch stage should be able to fetch a large block of code in one clock cycle, possibly crossing a cache line boundary. The decoder should be able to decode the first one or two instructions in a fecthed code block in the first clock cycle, and determine the starting points of the remaining instructions. This will allow it to decode maybe four or more instructions in the second clock cycle after fetching a block of code. The aim of this is to keep the branch misprediction delay as low as possible. This delay will be 3 clock cycles if there are three stages in the front end path: fetch, decode, and execute. This requires that the two decode stages are joined into one.
\vspace{2mm}

The front end may have access to a memory read port because some control transfer instructions have memory operands. Jump tables are particularly critical here. Jump tables are normally placed in the read-only memory block (see page \pageref{memoryModel}). The speed of access to jump tables may be increased by having a separate cache for read-only memory.
\vspace{2mm}

A special register is used for holding the information about which registers are marked for calculation in the front end. This special register has callee-save status. It may have a default value to specify certain registers that are normally used for loop counters, etc.
\vspace{2mm}

The demand for advanced branch prediction is somewhat relaxed if we can drastically reduce the branch misprediction delay with the measures described here. The branch prediction machinery in modern processors is very complicated and requires a lot of silicon space for branch target buffers and pattern/history tables. We can cut down on this and use a simpler branch prediction scheme if branches are resolved in the front end.
\vspace{2mm}

\end{document}
